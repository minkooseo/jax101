{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from typing import Any\n",
    "from jax import numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows BatchNorm that needs to maintain `batch_stats` variables. See `Flax basics.ipynb` for the case w/o variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_key, noise_key, model_key = jax.random.split(jax.random.PRNGKey(0), 3)\n",
    "xs = jax.random.normal(x_key, (100, 1))\n",
    "noise = jax.random.normal(noise_key, (100, 1))\n",
    "W, b = 2, -1\n",
    "ys = xs + noise + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer with a batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerDnnWithBatchNorm(nn.Module):\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool):\n",
    "        x = nn.Dense(512)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "class MyTrainState(train_state.TrainState):\n",
    "    batch_stats: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state: MyTrainState, xs, ys):\n",
    "    \n",
    "    def loss_fn(params):\n",
    "        # When there're variables, their updates is returned by model.apply.\n",
    "        yhats, updates = state.apply_fn(\n",
    "            {\n",
    "                'params': params, \n",
    "                'batch_stats': state.batch_stats # Variables are specified.\n",
    "            },\n",
    "            xs, \n",
    "            train=True, # For batch norm train vs eval\n",
    "            mutable=['batch_stats'] # For variables updates\n",
    "        )\n",
    "        loss = jnp.mean((ys - yhats) ** 2)\n",
    "        return loss, updates\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, updates), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    # Required to update the batch statistics.\n",
    "    state = state.replace(batch_stats=updates['batch_stats'])\n",
    "    metrics = {'loss': loss}\n",
    "    return state, metrics\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state: MyTrainState, xs, ys):\n",
    "    yhats, _ = state.apply_fn({'params': state.params}, xs, train=False)\n",
    "    loss = jnp.mean((ys - yhats) ** 2)\n",
    "    metrics = {'loss': loss}\n",
    "    return metrics        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss @ 0: 2.3187077045440674\n",
      "Loss @ 100: 1.0786985158920288\n",
      "Loss @ 200: 1.0493273735046387\n",
      "Loss @ 300: 1.0176451206207275\n",
      "Loss @ 400: 0.9789206981658936\n",
      "Loss @ 500: 0.9367572069168091\n",
      "Loss @ 600: 0.9019906520843506\n",
      "Loss @ 700: 0.8809128403663635\n",
      "Loss @ 800: 0.8683042526245117\n",
      "Loss @ 900: 0.8628705143928528\n",
      "Loss @ 1000: 0.8435438871383667\n"
     ]
    }
   ],
   "source": [
    "model = MultiLayerDnnWithBatchNorm()\n",
    "variables = model.init(model_key, xs, train=False)\n",
    "params, batch_stats = variables['params'], variables['batch_stats']\n",
    "\n",
    "state = MyTrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    batch_stats=batch_stats,\n",
    "    tx=optax.adam(1e-3),\n",
    ")\n",
    "\n",
    "for i in range(1001):\n",
    "    state, metrics = train_step(state, xs, ys)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Loss @ {i}: {metrics[\"loss\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BatchNorm_0': {'mean': (512,), 'var': (512,)}, 'BatchNorm_1': {'mean': (256,), 'var': (256,)}}\n",
      "{\n",
      "  \"BatchNorm_0\": {\n",
      "    \"mean\": [\n",
      "      -0.02687761001288891,\n",
      "      0.03383558616042137,\n",
      "      0.017856178805232048\n",
      "    ],\n",
      "    \"var\": [\n",
      "      1.5422340631484985,\n",
      "      2.033207893371582,\n",
      "      0.8356245160102844\n",
      "    ]\n",
      "  },\n",
      "  \"BatchNorm_1\": {\n",
      "    \"mean\": [\n",
      "      -1.114841341972351,\n",
      "      2.4079277515411377,\n",
      "      3.353090286254883\n",
      "    ],\n",
      "    \"var\": [\n",
      "      0.9122587442398071,\n",
      "      11.84886646270752,\n",
      "      5.084463119506836\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def pretty_dict(d):\n",
    "  return json.dumps(d, indent=2)\n",
    "\n",
    "print(jax.tree_util.tree_map(jnp.shape, state.batch_stats))\n",
    "print(pretty_dict(jax.tree_util.tree_map(lambda x: x.tolist()[:3], state.batch_stats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
